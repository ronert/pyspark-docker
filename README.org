
This Docker image helps you to run Spark (on Docker) with the following installed:

1. [[https://spark.apache.org/][pySpark]] (Spark 1.2.1) on Hadoop 2.6.0
2. python 2.6.6
3. numpy 1.9.0
4. scipy 0.14.0
5. scikit-learn 0.15.2

* Starting pyspark


** 1. Pull the docker image

Run the following commands in your MacOS terminal (make sure that
=$DOCKER_HOST= is set correctly)

#+begin_src 
docker pull smungee/pyspark-docker:latest
#+end_src


** 2. Start the container

Run the following command to start the container and get a bash prompt

#+begin_src sh
docker run -i -t -p 4040:4040 -h sandbox smungee/pyspark-docker:latest /etc/bootstrap.sh -bash
#+end_src

** 3. Start pyspark

#+begin_src 
/usr/local/spark/bin/pyspark
#+end_src

This should place you in a python prompt (=>>>=)
** 4. Verify installation

To verify pyspark, run the following example Spark program:
#+begin_src python
data = [1, 2, 3, 4, 5]
sc.parallelize(data).count()
#+end_src

This should print a bunch of debugging output, and on the last line,
it should print the output, "5"

To verify scikit-learn, run the following example program:

#+begin_src python
from sklearn import svm, datasets
clf = svm.SVC(gamma=0.001, C=100.)
digits = datasets.load_digits()
clf.fit(digits.data[:-1], digits.target[:-1])
#+end_src

You should see output like:
#+begin_src 
SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,
  random_state=None, shrinking=True, tol=0.001, verbose=False)
#+end_src

